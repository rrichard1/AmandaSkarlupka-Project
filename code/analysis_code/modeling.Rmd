---
title: "Modelling"
author: "Amanda Skarlupka"
date: "11/26/2019"
output: html_document
---
Figure 9, 10, 11, 12, 13

Tables 3, 4, 5, 6

Supplemental table 1

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library('tidyr')
library('forcats')
library('ggplot2')
library('knitr')
library('mlr') #for model fitting.
library('parallelMap') #for using multiple processors when running models through mlr
library('tidyverse')
library('here')
library('caret')
library('doParallel')
library('rpart')
library('rpart.plot')
library('mda')
library('ranger')
library('e1071')
library('readr')
library('dplyr')
library('ggpubr')

```

Load the data

```{r}
mabs <- readRDS(here::here("data", "processed_data", "mabs_processed_data.rds"))
mabs_full <- readRDS(here::here("data", "processed_data", "mabs_full.rds"))

```

Lets clean up the mabs file a little for analysis. Remove Key, raised_by, order_name, long_name, key_name, classification. Host and HA subtype only have one unique value so remove those. Replace the NA's in lineage and sublineage with unknown. Then change the characters into factors. 
```{r}

visdat::vis_dat(mabs)
visdat::vis_miss(mabs)

mabs <- mabs %>%
  select(-c(key, raised_by, order_name, long_name, key_name, classification, ha_sub, host))

mabs$sublineage[is.na(mabs$sublineage)] <- "Unknown"
mabs$lineage[is.na(mabs$lineage)] <- "Unknown"

mabs$sublineage <- as.factor(mabs$sublineage)
mabs$lineage <- as.factor(mabs$lineage)
mabs$antigen <- as.factor(mabs$antigen)
mabs$clone_name <- as.factor(mabs$clone_name)
mabs$na_sub <- as.factor(mabs$na_sub)
mabs$elisa_specificity <- as.factor(mabs$elisa_specificity)
mabs$raised_against <- as.factor(mabs$raised_against)



variables <- visdat::vis_dat(mabs)

ggsave(here::here("results", "figures", "visualizing_variables.png"), plot = variables)

```


```{r}

titers <- visdat::vis_dat(mabs_full)
visdat::vis_miss(mabs_full)

ggsave(here::here("results", "figures", "visualizing_titers.png"), plot = titers)
```
I am interested to see if I can tell based on HAI titer if it will be a P1 monoclonal antibody or a CA/09 antibody. 

So I'll take a look at the outcome variable...

```{r}
outcome_histo <- mabs_full %>%
  ggplot(aes(x = mab)) +
  geom_histogram(stat = "count")+
  labs(
    title = "Histogram of Outcome Varibale (Origin of Monoclonal)",
    x = "Monoclonal Antibody"
  )

ggsave(here::here("results", "figures", "outcome_histo.png"), plot = outcome_histo)
```

All of the predictors have all their data. So I want to make sure that they are correctly input and the data looks normal with no weird errors. I want to change the 'mab' into a factor. 

```{r}
mabs_full %>%
  ggplot(aes(x = `Spain/2003`)) +
  geom_bar()

str(mabs_full)
mabs_full$mab <- as.factor(mabs_full$mab)

predictors <- mabs_full %>% 
  gather(c(`Spain/2003`, `Zhejiang/07`, `Swine/31`, `Illinois/09`, `Minnesota/09`, `Nebraska/13`, `Iowa/73`, `WI/97`, `Colorado/09`, `NC/34543/09`, `MN/15`, `Utah/09`, `NC/09`, `Missouri/13`, `NC/15`, `Indiana/00`, `NC/01`, `NC/5043-1/09`), key = "var", value = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(binwidth = 2) +
  scale_x_continuous(trans='log2') +
  facet_wrap(~ var)

ggsave(here::here("results", "figures", "predictors_histogram.png"), plot = predictors)

str(mabs_full)
```

```{r}
outcome_context <- mabs_full %>% 
  gather(c(`Spain/2003`, `Zhejiang/07`, `Swine/31`, `Illinois/09`, `Minnesota/09`, `Nebraska/13`, `Iowa/73`, `WI/97`, `Colorado/09`, `NC/34543/09`, `MN/15`, `Utah/09`, `NC/09`, `Missouri/13`, `NC/15`, `Indiana/00`, `NC/01`, `NC/5043-1/09`), key = "var", value = "value") %>%
  ggplot(aes(x = mab, y = value)) +
  geom_violin() +
  facet_wrap(~ var, scales = 'free')

outcome_context
ggsave(here::here("results", "figures", "outcome_variable_distribution.png"), plot = outcome_context)


group_by(mabs, antigen, raised_against) %>%
  summarise(
    count = n(),
    mean = mean(dilution, na.rm = TRUE),
    sd = sd(dilution, na.rm = TRUE)
  )
ggboxplot(mabs, x = "raised_against", y = "concentration", 
          color = "raised_against", palette = c("#00AFBB", "#E7B800", "red", "blue", "green"),
        ylab = "Dilution", xlab = "Specific To")

with(mabs, shapiro.test(log2[raised_against == "ca09"])) # W = 0.66559, p-value < 2.2e-16.
with(mabs, shapiro.test(log2[raised_against == "p1"])) #W = 0.64129, p-value < 2.2e-16
#with(mabs, shapiro.test(log2[raised_against == "bris07"])) #Sample size is too small, needs to be greater than 2
with(mabs, shapiro.test(log2[raised_against == "sc18"])) #W = 0.7134, p-value = 5.899e-09
#with(mabs, shapiro.test(log2[raised_against == "pr34"])) #All the x values are identical

with(mabs, shapiro.test(dilution[raised_against == "ca09"])) #W = 0.38263, p-value < 2.2e-16
with(mabs, shapiro.test(dilution[raised_against == "p1"])) #W = 0.41656, p-value < 2.2e-16
#with(mabs, shapiro.test(dilution[raised_against == "bris07"])) #Sample size is too small, needs to be greater than 2
with(mabs, shapiro.test(dilution[raised_against == "sc18"])) #W = 0.71887, p-value = 7.51e-09
#with(mabs, shapiro.test(dilution[raised_against == "pr34"])) #All the x values are identical

ca09_normality <- with(mabs, shapiro.test(concentration[raised_against == "ca09"])) #W = 0.64754, p-value < 2.2e-16
p1_normality <- with(mabs, shapiro.test(concentration[raised_against == "p1"])) #W = 0.63063, p-value < 2.2e-16
#with(mabs, shapiro.test(concentration[raised_against == "bris07"])) #Sample size is too small, needs to be greater than 2
with(mabs, shapiro.test(concentration[raised_against == "sc18"])) #W = 0.65793, p-value = 5.947e-10
#with(mabs, shapiro.test(concentration[raised_against == "pr34"])) #All the x values are identical

saveRDS(here::here("results", "tables", "ca09_normality_results.rds"), object = ca09_normality)
saveRDS(here::here("results", "tables", "p1_normality_results.rds"), object = p1_normality)

```
My data doesn't pass the normality test. Therefore, it's suggested to do a non-parametric Wilcoxon rank test. The rank test arguements take two numeric vectors as x and y
```{r}
ca09_conc <- mabs %>%
  filter(raised_against == "ca09") %>%
  pull(concentration)
p1_conc <- mabs %>%
  filter(raised_against == "p1") %>%
  pull(concentration)
rank_results <- wilcox.test(ca09_conc, p1_conc, alternative = "two.sided") #W = 20292, p-value = 0.2867

saveRDS(here::here("results", "tables", "rank_results.rds"), object = rank_results)

ca09_log2 <- mabs %>%
  filter(raised_against == "ca09") %>%
  pull(log2)
p1_log2 <- mabs %>%
  filter(raised_against == "p1") %>%
  pull(log2)

wilcox.test(ca09_log2, p1_log2, alternative = "two.sided") #W = 20292, p-value = 0.2867


```
The rank tests for the log2 and concentration are the same result wise. I am not surprised by these results because they are just transformations of each other, so the individual data points never actually switch spots. 

I'll test for the variances being the same with the F-test.

```{r}
mabs_ftest <- mabs %>%
  filter(raised_against == c("ca09", "p1"))
log2.ftest <- var.test(log2 ~ raised_against, data = mabs_ftest)
concentration.ftest <- var.test(concentration ~ raised_against, data = mabs_ftest)
dilution.ftest <- var.test(dilution ~ raised_against, data = mabs_ftest)
log2.ftest #ratio of variances: 1.219726 p-value = 0.3058
f_test <- concentration.ftest #ratio of variances: 0.9692263, p-value = 0.8719
dilution.ftest #ratio of variances: 5.403161, p-value < 2.2e-16

saveRDS(here::here("results", "tables", "f_test.rds"), object = f_test)
```
So the log2 and concentrations have the same variance, but the dilution does not. 

```{r}

mabs_ttest <- mabs_full %>% 
  select(mab, `Spain/2003`, `Zhejiang/07`, `Swine/31`) %>%
  gather(key = variable, value = value, -mab) 

one_three <- mabs_ttest %>%
  group_by(mab, variable) %>% 
  summarise(value = list(value)) %>% 
  spread(mab, value) %>% 
  group_by(variable) %>% 
 mutate(p_value = t.test(unlist(ca09), unlist(p1))$p.value,
        t_value = t.test(unlist(ca09), unlist(p1))$statistic)
print(one_three)
```

```{r}
#mabs_ttest <- mabs_full %>% 
#  select(mab, `Illinois/09`, `Minnesota/09`) %>%
#  gather(key = variable, value = value, -mab) 

#mabs_ttest %>%
#  group_by(mab, variable) %>% 
# summarise(value = list(value)) %>% 
#  spread(mab, value) %>% 
#  group_by(variable) %>% 
# mutate(p_value = t.test(unlist(ca09), unlist(p1))$p.value,
#        t_value = t.test(unlist(ca09), unlist(p1))$statistic)
```
I'm getting a lot of errors with the t-testing because the data doesn't follow a normal distribution. 



```{r}
mabs_full <- mabs_full[,c(2,1,3:18)] ##number of columns needed to be changed -megan
##mabs_full <- mabs_full %>%
##  select(-clone_name)
## clone name already gone? -megan
visdat::vis_dat(mabs_full)
correlation <- visdat::vis_cor(mabs_full[,c(2:18)])
ggsave(here::here("results", "figures", "correlations.png"), plot = correlation)
ggsave(here::here("results", "figures", "predictors_outcome.png"), plot = outcome_context)
```

# Setup

Some setup settings that are used in various code chunks below. 

```{r mlr-setup}
outcome <- mabs_full$mab
outcomename = "mab"
predictors <- mabs_full[,-1]
npred=ncol(predictors)
#set sampling method for performance evaluation
#here, we use 5-fold cross-validation, 5-times repeated
sampling_choice = makeResampleDesc("RepCV", reps = 5, folds = 5)
```

```{r parallel}
n_cores <- 4 #number of cores to use
cl <- makePSOCKcluster(n_cores)
registerDoParallel(cl) #comment out this line if you don't want parallel computing
```




## A null model

To define a null model, we need to determine what performance measure we want to track. As mentioned in the course materials, there are different performance measures. Accuracy or misclassification error is simple, it just counts the number of times the model got it right/wrong. We'll start with that one, and then try another one later. `mlr` allows for a lot of different performance measures for both categorical and continuous outcomes, see [here](https://mlr.mlr-org.com/articles/tutorial/performance.html) and [here](https://mlr.mlr-org.com/articles/tutorial/measures.html).

For accuracy, the simplest null model always predicts the most frequent category. We can use that as baseline performance.


```{r nullmodel-acc}
#write code that computes accuracy for a null model
table(mabs_full$mab)

#the null model always predicts "ca09" because this is the most frequent category of our expected outcome. 

measureACC(mabs_full$mab, "ca09")

```

The null model returns a ACC of 0.6 for the prediction of CA/09. 

## Single predictor models

Now let's consider single predictor models, i.e. we'll fit the outcome to each predictor one at a time to get an idea of the importance of individual predictors. To evaluate our model performance, we will use cross-validation. Since our outcome is categorical, we'll use a logistic model. CA09 is set to the positive because it is the ACC null predictor value

```{r, unifit, warning=FALSE}
set.seed(1111) #makes each code block reproducible
#set learner/model. this corresponds to a logistic model.
#mlr calls different models different "learners"
learner_name = "classif.binomial";
mylearner = makeLearner(learner_name, predict.type = "prob")
# this will contain the results
unifmat=data.frame(variable = rep(0,npred), Accuracy = rep(0,npred))
# loop over each predictor, build simple dataset with just outcome and that predictor, fit it to a glm/logistic model
for (nn in 1:npred)
{
    unidata = data.frame(mab = outcome, mabs_full[,nn+1] )
    ## Generate the task, i.e. define outcome and predictors to be fit
    mytask = makeClassifTask(id='unianalysis', data = unidata, target = outcomename, positive = "ca09")
    model = resample(mylearner, task = mytask, resampling = sampling_choice, show.info = FALSE, measures = acc )
    unifmat[nn,1] = names(predictors)[nn] 
    unifmat[nn,2] = model$aggr
}
single_predictor <- kable(unifmat)
saveRDS(here::here("results", "tables", "single_predictor.rds"), object = single_predictor)
```

So looks like only one of the single predictor models have a higher accuracy than the null. That one is the NC/09 HA with a value of 0.833. Maybe next I'll want to look if any other features would make good predictors. 

# Full model

Now let's fit a full logistic model with all predictors. 

I'm running into the issue where the column names aren't following Rs naming conventions. So I need to rename the columns and remove the "/". 
```{r}

colnames(mabs_full)

mabs_full <- mabs_full %>%
  rename(spain_03 = `Spain/2003`, zhejiang_07 = `Zhejiang/07`, swine_31 = `Swine/31`, il_09 = `Illinois/09`, mn_09 = `Minnesota/09`, ne_13 = `Nebraska/13`, ia_73 = `Iowa/73`, wi_97 = `WI/97`, co_09 = `Colorado/09`, nc_34543_09 = `NC/34543/09`, mn_15 = `MN/15`, ut_09 = `Utah/09`, nc_09 = `NC/09`, mo_13 = `Missouri/13`, nc_15 = `NC/15`, in_09 = `Indiana/00`, nc_01 = `NC/01`, nc_5043_09 = `NC/5043-1/09`)

```


```{r fullfit}
set.seed(1111) #makes each code block reproducible
#do full model with Cross-Validation - to get an idea for the amount of over-fitting a full model does
mytask = makeClassifTask(id='fullanalysis', data = mabs_full, target = outcomename, positive = "ca09")
fullmodel = resample(mylearner, task = mytask, resampling = sampling_choice, show.info = FALSE, measures = acc )
ACC_fullmodel = fullmodel$aggr[1]
full <- print(ACC_fullmodel)

saveRDS(object = full, here("results", "tables", "full_logistic_model.rds"))
```

The value is 0.6. 

Now let's do subset selection. The code below does it several ways. It does regular forward and backward selection and floating versions of those. It also uses a genetic algorithm for selection. See the `mlr` website [here](https://mlr.mlr-org.com/articles/tutorial/feature_selection.html) for details.

Also note that I included a kind of timer in the code, to see how long things take. That's a good idea if you run bigger models. You first run a few iterations on maybe a few cores, then you can compute how long it would take if you doubled the iterations, or doubled the cores, etc. This prevents bad surprises of trying to _quickly_ run a piece of code and waiting hours. You should always use short runs to make sure everything works in principle, and only at the end do the real, long "production" runs. Otherwise you might waste hours/days/weeks waiting for results only to realize that you made a basic mistake and you have to do it all over.

```{r mlr-subset selection}
set.seed(1111) 
tstart=proc.time(); #capture current CPU time for timing how long things take
#do 2 forms of forward and backward selection, just to compare
select_methods=c("sbs","sfbs","sfs","sffs") 
resmat=data.frame(method = rep(0,4), Accuracy = rep(0,4), Model = rep(0,4))
ct=1;
for (select_method in select_methods) #loop over all stepwise selection methods
{
  ctrl = makeFeatSelControlSequential(method = select_method)
  print(sprintf('doing subset selection with method %s ',select_method))
  sfeat_res = selectFeatures(learner = mylearner, 
                             task = mytask, 
                             resampling = sampling_choice, 
                             control = ctrl, 
                             show.info = FALSE,
                             measures = acc)
  
  resmat[ct,1] = select_methods[ct]
  resmat[ct,2] = sfeat_res$y
  resmat[ct,3] = paste(as.vector(sfeat_res$x), collapse= ', ')
  ct=ct+1;
}
# do feature selection with genetic algorithm
maxit = 100 #number of iterations - should be large for 'production run'
ctrl_GA =makeFeatSelControlGA(maxit = maxit)
print(sprintf('doing subset selection with genetic algorithm'))
sfeatga_res = selectFeatures(learner = mylearner, 
                                   task = mytask, 
                                   resampling = sampling_choice, 
                                   control = ctrl_GA, 
                                   show.info = FALSE,
                                   measures = acc)
resmat[5,1] = "GA"
resmat[5,2] = sfeatga_res$y
resmat[5,3] = paste(as.vector(sfeatga_res$x), collapse= ', ')
runtime.minutes_SS=(proc.time()-tstart)[[3]]/60; #total time in minutes the optimization took
print(sprintf('subset selection took %f minutes',runtime.minutes_SS));
subset_selection <- kable(resmat)

saveRDS(object = subset_selection, file = here("results", "tables", "subset_selection.rds"))
```

The subset selection took 4.1 minutes for 100 interations. From the sub-modelling the performance is better than seen before (0.6). 

```{r}
str(mabs)

var <- c("clone_name", "antigen", "elisa_specificity", 'virus', 'elisa_specificity', 'lineage', 'sublineage')


mabs %>% 
  gather(c("clone_name", "antigen", "elisa_specificity", 'virus', 'elisa_specificity', 'lineage', 'sublineage'), key = "var", value = "value") %>%
  ggplot(aes(x = value)) +
  geom_histogram(stat = "count")+
  facet_wrap(~ var)

different_predictors <- mabs %>% 
  gather(c(clone_name, antigen, elisa_specificity, virus, lineage, sublineage), key = "var", value = "value") %>%
  ggplot(aes(x = raised_against, y = value)) +
  geom_count() +
  facet_wrap(~ var, scales = 'free')

different_predictors

ggsave(here::here("results", "figures", "supp_different_predictors.png"), plot = different_predictors, width = 8, height = 9)
```

I'll remove antigen, clone_name, and Elisa specificity because they don't have values for some of the predictors, and in the case of the antigen they are all equal. 

```{r}
mabs <- mabs %>%
  select(-c(antigen, clone_name, elisa_specificity))
cleaned_predictors <- mabs %>% 
  gather(c(virus, lineage, sublineage), key = "var", value = "value") %>%
  ggplot(aes(x = raised_against, y = value)) +
  geom_count() +
  facet_wrap(~ var, scales = 'free')

ggsave(here::here("results", "figures", "different_predictors.png"), plot = cleaned_predictors, width = 8, height = 4)

mabs <- mabs[, c(4, 1:3, 5:8)]
mabs$virus <- as.factor(mabs$virus)

visdat::vis_dat(mabs)

```
# Setup

Some setup settings that are used in various code chunks below. 

```{r mlr-setup}
outcome <- mabs$raised_against
outcomename = "raised_against"
predictors <- mabs[,-1]
npred=ncol(predictors)
#set sampling method for performance evaluation
#here, we use 5-fold cross-validation, 5-times repeated
sampling_choice = makeResampleDesc("RepCV", reps = 5, folds = 5)
```



## A null model

To define a null model, we need to determine what performance measure we want to track. As mentioned in the course materials, there are different performance measures. Accuracy or misclassification error is simple, it just counts the number of times the model got it right/wrong. We'll start with that one, and then try another one later. `mlr` allows for a lot of different performance measures for both categorical and continuous outcomes, see [here](https://mlr.mlr-org.com/articles/tutorial/performance.html) and [here](https://mlr.mlr-org.com/articles/tutorial/measures.html).

For accuracy, the simplest null model always predicts the most frequent category. We can use that as baseline performance.



```{r nullmodel-acc}
#write code that computes accuracy for a null model
table(mabs$raised_against)

#the null model always predicts "P1" because this is the most frequent category of our expected outcome. 

measureACC(mabs$raised_against, "p1")

```

The null model returns a ACC of 0.4 for the prediction of P1. 

Tree of mabs

```{r parallel}
n_cores <- 4 #number of cores to use
cl <- makePSOCKcluster(n_cores)
registerDoParallel(cl) #comment out this line if you don't want parallel computing
```
I don't have enough data to do a split so I'm just going to keep it all together. We know from above that the null model accuracy of predicting P1 (the greatest value) is 0.4 (216/540 total observations). 


## Single predictor models

Now let's consider single predictor models, i.e., we'll fit the outcome to each predictor one at a time to get an idea of the importance of individual predictors. Here, our model will be a tree. I'm actually not so sure if this makes a lot of sense since a "tree" with only one predictor seems a bit silly. But I guess we can try. It's similar to a 1-predictor GLM. 

We'll also do some parameter tuning here. Looking at the [caret documentation](http://topepo.github.io/caret/available-models.html), we find that the tuning parameter for the `rpart` model (which is the tree algorithm) is called `cp`. We could also find that using `modelLookup("rpart")`. We could either specify a grid of values to try for `cp` (we'll use a grid below), or, for a single tuning parameter, `caret` allows one to set the number of values to try and picks those values automatically. We'll do the latter approach here.



```{r singlepredictor}
#There is probably a nicer tidyverse way of doing this. I just couldn't think of it, so did it this way.
set.seed(1111) #makes each code block reproducible
outcomename = "raised_against"
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) #setting CV method for caret
Npred <- ncol(mabs)-1 # number of predictors
resultmat <- data.frame(Variable = names(mabs)[-1], Accuracy = rep(0,Npred)) #store performance for each variable
for (n in 2:ncol(mabs)) #loop over each predictor. For this to work, outcome must be in 1st column
{
  fit1 <- train( as.formula(paste(outcomename, "~",names(mabs)[n])) , data = mabs, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 10) 
resultmat[n-1,2]= max(fit1$results$Accuracy)  
}
tree_single_predictor <- print(resultmat)
saveRDS(object = tree_single_predictor, file = here("results", "tables", "tree_single_predictor.rds"))
```
Some of concentration, log2, and dilution ( which are just transformations of each other) have about the same accuracy of 0.44, marginally better than the null. 


```{r fullfit}
set.seed(1111) #makes each code block reproducible
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 
fit1 = train(raised_against  ~ ., data=mabs, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 10) 
print(fit1$results)
```

The accuracy isn't looking much better. 

```{r printfigure, message=FALSE}
prp(fit1$finalModel, extra = 1, type = 1)
ww=17.8/2.54; wh=ww; #for saving plot
dev.print(device=png,width=ww,height=wh,units="in",res=600,file= here("results", "figures", "rparttree.png")) #save tree to file
```

I don't think I have enough data for a tree. 

```{r stop parallel}
# shut down the parallel computing cluster we created at the beginning of the analysis
stopCluster(cl)
```